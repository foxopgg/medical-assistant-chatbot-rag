{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Dict\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d717cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"data/medical_text_data.csv\"   # your file\n",
    "INDEX_DIR = \"faiss_index\"\n",
    "EMBED_MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"  # multilingual, compact\n",
    "EMBED_DIM = 384  # for the MiniLM family\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e263fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(INDEX_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c1f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prepare(csv_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Read CSV and convert each row into a Document with combined text.\n",
    "    Expect either columns [instruction,response] or any columns; we'll combine all text fields.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    rows_text = []\n",
    "    for _, r in df.iterrows():\n",
    "        # combine textual columns\n",
    "        text_fields = []\n",
    "        for c in df.columns:\n",
    "            val = r[c]\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            text_fields.append(f\"{c}: {val}\")\n",
    "        full = \" | \".join(text_fields)\n",
    "        # simple clean\n",
    "        full = re.sub(r\"\\s+\", \" \", str(full)).strip()\n",
    "        rows_text.append(full)\n",
    "    # chunk texts\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\" \",\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "    docs = []\n",
    "    for i, t in enumerate(rows_text):\n",
    "        pieces = splitter.split_text(t)\n",
    "        for j, p in enumerate(pieces):\n",
    "            docs.append(Document(page_content=p, metadata={\"source_row\": i}))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803293cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings_and_faiss(docs: List[Document], model_name: str = EMBED_MODEL_NAME):\n",
    "    print(\"Loading embedding model:\", model_name)\n",
    "    # Use sentence-transformers directly (fast and compact)\n",
    "    st_model = SentenceTransformer(model_name)\n",
    "    texts = [d.page_content for d in docs]\n",
    "    print(\"Embedding\", len(texts), \"chunks ...\")\n",
    "    embeddings = st_model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "    # build FAISS index (Flat L2). For bigger scale consider HNSW/IVF+PQ\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    print(\"Adding embeddings to FAISS index...\")\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "\n",
    "    # Save the index and metadata\n",
    "    faiss.write_index(index, os.path.join(INDEX_DIR, \"index.faiss\"))\n",
    "    with open(os.path.join(INDEX_DIR, \"docs.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(docs, f)\n",
    "    print(\"Saved FAISS index and docs.\")\n",
    "    return index, docs, st_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87d4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_and_docs():\n",
    "    idx_path = os.path.join(INDEX_DIR, \"index.faiss\")\n",
    "    docs_path = os.path.join(INDEX_DIR, \"docs.pkl\")\n",
    "    if not os.path.exists(idx_path) or not os.path.exists(docs_path):\n",
    "        raise FileNotFoundError(\"Index or docs not found. Run build first.\")\n",
    "    index = faiss.read_index(idx_path)\n",
    "    with open(docs_path, \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "    return index, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3ecbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_langchain_faiss_vectorstore(docs: List[Document], hf_model_name=EMBED_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Wrap FAISS into LangChain VectorStore for ease of retrieval with chains.\n",
    "    This uses HuggingFaceEmbeddings (thin wrapper) pointing to sentence-transformers.\n",
    "    \"\"\"\n",
    "    # LangChain wrapper using HuggingFaceEmbeddings consumes a model name; it expects transformers-compatible model.\n",
    "    # We'll use the same model but via HuggingFaceEmbeddings for compatibility.\n",
    "    embed = HuggingFaceEmbeddings(model_name=hf_model_name)\n",
    "    # create FAISS vectorstore from texts + docs\n",
    "    texts = [d.page_content for d in docs]\n",
    "    metadatas = [d.metadata for d in docs]\n",
    "    vect = FAISS.from_texts(texts, embed, metadatas=metadatas)\n",
    "    # save\n",
    "    vect.save_local(INDEX_DIR)\n",
    "    return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfea4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_query(vect_store):\n",
    "    \"\"\"\n",
    "    Make a retrievalQA chain using Ollama as LLM (local).\n",
    "    Make sure Ollama server is running and the model is pulled (e.g., 'llama3' or a small variant).\n",
    "    \"\"\"\n",
    "    # LLM via Ollama - uses local Ollama server (default host: http://localhost:11434)\n",
    "    # In LangChain new versions, use Ollama wrapper:\n",
    "    llm = Ollama(model=\"llama3:instruct\")  # adjust the model tag you pulled in ollama\n",
    "    retriever = vect_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"You are a medical assistant that helps identify dental/tooth-related issues and gives safe, brief treatment guidance. \"\n",
    "        \"User question: {question}\\n\\n\"\n",
    "        \"Use retrieved context to answer. If not confident, say you are not sure and suggest seeing a dentist.\"\n",
    "    )\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "    print(\"You can now ask questions. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        q = input(\"\\nUser query> \")\n",
    "        if q.strip().lower() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "        res = qa.run(q)\n",
    "        print(\"\\nAnswer:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9222af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Build docs & index (if not already built)\n",
    "    if not os.path.exists(os.path.join(INDEX_DIR, \"index.faiss\")):\n",
    "        print(\"Reading CSV and building documents...\")\n",
    "        documents = read_and_prepare(CSV_PATH)\n",
    "        index, docs, st_model = build_embeddings_and_faiss(documents)\n",
    "        # Optionally build LangChain FAISS wrapper for easier querying via langchain\n",
    "        vect = build_langchain_faiss_vectorstore(docs)\n",
    "    else:\n",
    "        print(\"FAISS index exists. Loading...\")\n",
    "        # load via langchain wrapper\n",
    "        from langchain.vectorstores import FAISS as LCFAISS\n",
    "        vect = LCFAISS.load_local(INDEX_DIR, HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME))\n",
    "\n",
    "    # Step 2: Run RAG interactive loop (Ollama must be running)\n",
    "    run_rag_query(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a693cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading FAISS index and docs.pkl ...\n",
      "✅ Loaded 246945 documents\n",
      "🎉 Done! Converted to LangChain format (index.pkl created)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "INDEX_DIR = \"faiss_index\"\n",
    "EMBED_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# 1. Load old index + docs\n",
    "print(\"📂 Loading FAISS index and docs.pkl ...\")\n",
    "faiss_index = faiss.read_index(f\"{INDEX_DIR}/index.faiss\")\n",
    "with open(f\"{INDEX_DIR}/docs.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "print(f\"✅ Loaded {len(docs)} documents\")\n",
    "\n",
    "# 2. Wrap existing index + docs (NO re-embedding)\n",
    "embed = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "docstore = InMemoryDocstore({str(i): d for i, d in enumerate(docs)})\n",
    "index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n",
    "\n",
    "vect = FAISS(\n",
    "    embedding_function=embed,\n",
    "    index=faiss_index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    ")\n",
    "\n",
    "# 3. Save properly (creates index.pkl alongside index.faiss)\n",
    "vect.save_local(INDEX_DIR)\n",
    "print(\"🎉 Done! Converted to LangChain format (index.pkl created)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65f6f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "INDEX_DIR = \"faiss_index\"\n",
    "EMBED_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "embed = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "vect = FAISS.load_local(\n",
    "    INDEX_DIR,\n",
    "    embed,\n",
    "    allow_dangerous_deserialization=True  # ✅ required for pickle\n",
    ")\n",
    "\n",
    "print(\"✅ FAISS index loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "INDEX_DIR = \"faiss_index\"\n",
    "EMBED_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "instructions = {\n",
    "    \"en\": \"Answer ONLY using the medical context below. If the answer is not present, reply 'I don’t know'.\",\n",
    "    \"or\": \"କୃପୟା ନିମ୍ନୋଲିଖିତ ପ୍ରସଙ୍ଗରୁ ମାତ୍ର ଉତ୍ତର ଦିଅନ୍ତୁ | ଉତ୍ତର ନଥିଲେ 'ମୁଁ ଜାଣିନି' କୁହନ୍ତୁ |\",  # Odia\n",
    "    \"mr\": \"फक्त खालील संदर्भ वापरून उत्तर द्या. उत्तर उपलब्ध नसेल तर 'मला माहित नाही' असे म्हणा.\",  # Marathi\n",
    "    \"ur\": \"صرف نیچے دیے گئے سیاق و سباق کا استعمال کرتے ہوئے جواب دیں۔ اگر جواب موجود نہیں ہے تو 'مجھے نہیں معلوم' کہیں۔\",  # Urdu\n",
    "    \"ta\": \"கீழே உள்ள சூழலை மட்டுமே பயன்படுத்தி பதிலளிக்கவும். பதில் இல்லாவிட்டால் 'எனக்கு தெரியவில்லை' என்று சொல்லவும்.\",  # Tamil\n",
    "    \"te\": \"క్రింద ఇచ్చిన సందర్భం ఆధారంగా మాత్రమే సమాధానం ఇవ్వండి. సమాధానం లేకపోతే 'నాకు తెలియదు' అని చెప్పండి.\",  # Telugu\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Build RAG pipeline\n",
    "# -------------------------\n",
    "def load_vectorstore():\n",
    "    embed = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "    vect = FAISS.load_local(INDEX_DIR, embed, allow_dangerous_deserialization=True)\n",
    "    return vect\n",
    "\n",
    "def build_prompt(query, lang):\n",
    "    instruction_text = instructions.get(lang, instructions[\"en\"])\n",
    "    return f\"\"\"\n",
    "You are a multilingual medical assistant.\n",
    "{instruction_text}\n",
    "Always answer in the SAME language as the question.\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def build_chain(vect, query, lang):\n",
    "    llm = Ollama(model=\"phi3:mini\")  # or smaller llama3 variant if GPU is limited\n",
    "    retriever = vect.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    prompt = PromptTemplate.from_template(build_prompt(query, lang))\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa\n",
    "\n",
    "# -------------------------\n",
    "# Interactive Loop\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    vect = load_vectorstore()\n",
    "    print(\"✅ Chatbot ready. Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\n👤 You: \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            lang = detect(query)\n",
    "        except:\n",
    "            lang = \"en\"\n",
    "\n",
    "        qa = build_chain(vect, query, lang)\n",
    "        result = qa.invoke({\"query\": query})\n",
    "\n",
    "        print(\"\\n🤖 Bot:\", result[\"result\"])\n",
    "        print(\"📎 Sources:\", [d.metadata for d in result[\"source_documents\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff621e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langdetect import detect\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "INDEX_DIR = \"faiss_index\"\n",
    "EMBED_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "instructions = {\n",
    "    \"en\": \"Answer ONLY using the medical context below. If the answer is not present, reply 'I don’t know'.\",\n",
    "    \"or\": \"କୃପୟା ନିମ୍ନୋଲିଖିତ ପ୍ରସଙ୍ଗରୁ ମାତ୍ର ଉତ୍ତର ଦିଅନ୍ତୁ | ଉତ୍ତର ନଥିଲେ 'ମୁଁ ଜାଣିନି' କୁହନ୍ତୁ |\",  # Odia\n",
    "    \"mr\": \"फक्त खालील संदर्भ वापरून उत्तर द्या. उत्तर उपलब्ध नसेल तर 'मला माहित नाही' असे म्हणा.\",  # Marathi\n",
    "    \"ur\": \"صرف نیچے دیے گئے سیاق و سباق کا استعمال کرتے ہوئے جواب دیں۔ اگر جواب موجود نہیں ہے تو 'مجھے نہیں معلوم' کہیں۔\",  # Urdu\n",
    "    \"ta\": \"கீழே உள்ள சூழலை மட்டுமே பயன்படுத்தி பதிலளிக்கவும். பதில் இல்லாவிட்டால் 'எனக்கு தெரியவில்லை' என்று சொல்லவும்.\",  # Tamil\n",
    "    \"te\": \"క్రింద ఇచ్చిన సందర్భం ఆధారంగా మాత్రమే సమాధానం ఇవ్వండి. సమాధానం లేకపోతే 'నాకు తెలియదు' అని చెప్పండి.\",  # Telugu\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Helper functions\n",
    "# -------------------------\n",
    "def load_vectorstore():\n",
    "    embed = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "    vect = FAISS.load_local(INDEX_DIR, embed, allow_dangerous_deserialization=True)\n",
    "    return vect\n",
    "\n",
    "def build_prompt(query, lang):\n",
    "    instruction_text = instructions.get(lang, instructions[\"en\"])\n",
    "    return f\"\"\"\n",
    "You are a multilingual medical assistant.\n",
    "{instruction_text}\n",
    "Always answer in the SAME language as the question.\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def get_answer(query, vect):\n",
    "    try:\n",
    "        lang = detect(query)\n",
    "    except:\n",
    "        lang = \"en\"\n",
    "\n",
    "    llm = Ollama(model=\"phi3:mini\")  # ✅ lightweight model for frontend\n",
    "    retriever = vect.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    prompt = PromptTemplate.from_template(build_prompt(query, lang))\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa.invoke({\"query\": query})\n",
    "\n",
    "# -------------------------\n",
    "# Streamlit UI\n",
    "# -------------------------\n",
    "st.set_page_config(page_title=\"🩺 Medical Assistant Chatbot\", layout=\"wide\")\n",
    "\n",
    "st.title(\"🩺 Multilingual Medical Chatbot\")\n",
    "st.markdown(\"⚠️ **Disclaimer:** This chatbot is for informational purposes only. Please consult a doctor for real medical advice.\")\n",
    "\n",
    "vect = load_vectorstore()\n",
    "\n",
    "user_query = st.text_input(\"Enter your symptoms or question:\")\n",
    "\n",
    "if st.button(\"Ask\") and user_query.strip():\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        result = get_answer(user_query, vect)\n",
    "        st.markdown(f\"**🤖 Bot:** {result['result']}\")\n",
    "        with st.expander(\"📎 Sources\"):\n",
    "            for doc in result[\"source_documents\"]:\n",
    "                st.write(doc.metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
